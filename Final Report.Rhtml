<html>

<head>
<title>Introduction:</title>
</head>

<body>

<p>As has been stated in the description of the practical machine learning project this project consists of data collected from various wearable fitness devices as such as FitBit, JawBone, Nike Fuelband etc.. All this data is categorized as Human Activity Recognition and as a part of this they have tried to qualify instead of quantifying the workouts being performed by the users. In the model we are constructing they have collected information about the way people left dumbbells as a part of their exercise routine using various movement coordinate variables. The lifting of the dumbbell is then classified into 5 factors (or Classes)  namely 
(i)	The correct method (Class A)
(ii)	throwing the elbows to the front (Class B), 
(iii)	lifting the dumbbell only halfway (Class C),
(iv)	lowering the dumbbell only halfway (Class D) 
(v)	throwing the hips to the front (Class E). 

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3gxdJ2qD2 
</p>
<strong><u>Preprocessing of the data:</u></strong>
</p>
In order to begin with the data analysis we start by loading the required libraries and then loading the given datasets.

</p>

<!--begin.rcode
###Loading libraries
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
###Reading in the datasets

modeldata<-read.csv("pml-training.csv",na.strings = c("NA", ""))
modeldataTest<-read.csv("pml-testing.csv",na.strings = c("NA", ""))

end.rcode-->

<p>Then we clean the datasets by reducing the number of unwanted variables. We get rid of those which consists of mostly missing values and also eliminate certain numeric variables like date timestamp which do not add much value to this particular predictive model.</p>

<!--begin.rcode
###Cleaning the dataset to remove unwanted variables and thereby improving computing efficiency
summary(modeldata)
summary(modeldata$classe)

###Removing variables which has missing values for all the rows
rm_var = sapply(Training2, function(x) {sum(is.na(x))})
table(rm_var)
rm_columns = names(rm_var[rm_var==13436]) ###100 of the columns have 13436 rows with missing values - this was the result of the table function
Training2 = Training2[, !names(Training2) %in% rm_columns]
str(Training2)

###Removing other unwanted numeric variables such as timestamp

Training2<-Training2[c(-1,-3,-4,-5)]
end.rcode-->
<p>
<strong><u>Cross Validation of the data:</u></strong>
We split up the given cleaned dataset into testing and training sets and use a 70:30 split, in order to cross validate our predictive model.
</p>
<!--begin.rcode
###Partitioning the Training dataset into 70% training and 30% testing sets
inTrain <- createDataPartition(y=modeldata$classe, p=0.7, list=FALSE)
Training2 <- modeldata[inTrain, ]
Testing2 <- modeldata[-inTrain, ]
end.rcode-->
<strong><u>Predictive Models: </u></strong>
<p>We now use Random Forest and Decision Trees machine leaning algorithms to predict the class of the weight lift depending on the various accelerometer readings.</p>
<!--begin.rcode 
###Predicting using ML packages
###Decision Trees
set.seed(333)
modelfitDT <- rpart(classe ~ ., data=Training2, method="class")
rpart.plot(modelfitDT)
predictDT<- predict(modelfitDT,Testing2,type = "class")
confusionMatrix(predictDT, Testing2$classe)

###Random Forests - with cross validation
modelfitRF<-train(Training2$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 4), data=Training2)
print(modelfitRF, digits=3)
predictRF<- predict(modelfitRF,newdata = Testing2)
confusionMatrix(predictRF, Testing2$classe)  
end.rcode-->
<p>Here is the plot of the decision tree. <p>
<!--begin.rcode  fig.width=7, fig.height=6
rpart.plot(modelfitDT)
end.rcode-->

<p>The results using the Decision tree (Rpart) algorithm
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1504  101    0   13    6
         B   40  842   71   86   56
         C   22   77  849   48    6
         D   96   65   99  730   82
         E   12   54    7   87  932

Overall Statistics
                                          
               Accuracy : 0.8253          
                 95% CI : (0.8154, 0.8349)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.7794          
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.8984   0.7392   0.8275   0.7573   0.8614
Specificity            0.9715   0.9467   0.9685   0.9305   0.9667
Pos Pred Value         0.9261   0.7689   0.8473   0.6810   0.8535
Neg Pred Value         0.9601   0.9380   0.9638   0.9514   0.9687
Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
Detection Rate         0.2556   0.1431   0.1443   0.1240   0.1584
Detection Prevalence   0.2760   0.1861   0.1703   0.1822   0.1856
Balanced Accuracy      0.9350   0.8430   0.8980   0.8439   0.9140

The results using the Random Forest algorithm

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1673    2    0    0    0
         B    0 1136    1    0    0
         C    0    1 1025    5    0
         D    0    0    0  959    3
         E    1    0    0    0 1079

Overall Statistics
                                          
               Accuracy : 0.9978          
                 95% CI : (0.9962, 0.9988)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9972          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9994   0.9974   0.9990   0.9948   0.9972
Specificity            0.9995   0.9998   0.9988   0.9994   0.9998
Pos Pred Value         0.9988   0.9991   0.9942   0.9969   0.9991
Neg Pred Value         0.9998   0.9994   0.9998   0.9990   0.9994
Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
Detection Rate         0.2843   0.1930   0.1742   0.1630   0.1833
Detection Prevalence   0.2846   0.1932   0.1752   0.1635   0.1835
Balanced Accuracy      0.9995   0.9986   0.9989   0.9971   0.9985
<p>
<p>
Since the Random Forest yileds a higher accuracy we'll be using the same for predicting on the final test set.
<p>
<!--begin.rcode
predictFinal<- predict(modelfitRF,newdata = modeldataTest)
end.rcode-->
<p>The script for submitting the Final Assignment.<p>
<!--begin.rcode
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictFinal)
end.rcode-->
</body>
</html>
